{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Does Profiling Preference Data Improve DPO Training?\n\n**Hypothesis**: Filtering low-quality pairs (easy pairs, near-duplicates, low-contrast) from a preference dataset before DPO training yields better reward accuracy than training on the full unfiltered dataset.\n\n**Setup**: Qwen2.5-1.5B-Instruct, LoRA (r=16), UltraFeedback 5K subset, T4 GPU (Colab free tier).\n\n**Method**: Profile with preflight → filter flagged pairs → train DPO on full vs. filtered → compare reward accuracy + loss on held-out eval set."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install git+https://github.com/yudduy/preflight.git -q\n!pip install trl transformers datasets peft accelerate bitsandbytes -q",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Dataset + Train/Eval Split"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from datasets import load_dataset\nimport json\n\nds = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs[:5000]\")\nprint(f\"Loaded {len(ds)} preference pairs\")\n\n# Hold out 200 for evaluation\nds_train = ds.select(range(200, len(ds)))\nds_eval = ds.select(range(200))\nprint(f\"Train: {len(ds_train)}, Eval: {len(ds_eval)}\")\n\n# Save train split as JSONL for preflight\nwith open(\"ultrafeedback_train.jsonl\", \"w\") as f:\n    for row in ds_train:\n        f.write(json.dumps({\n            \"prompt\": row[\"prompt\"],\n            \"chosen\": row[\"chosen\"],\n            \"rejected\": row[\"rejected\"],\n        }) + \"\\n\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Run Preflight Audit"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!preflight audit ultrafeedback_train.jsonl --output audit_report.json\n\nimport json\nwith open(\"audit_report.json\") as f:\n    report = json.load(f)\n\nprint(\"\\n=== Audit Summary ===\")\nprint(f\"Samples: {report['metadata']['n_samples']}\")\nprint(f\"Length biased: {report['length_bias']['biased']} (p={report['length_bias']['p_value']:.4f})\")\nprint(f\"Low-contrast pairs: {report['embedding_similarity']['low_contrast_count']}\")\nprint(f\"Easy pairs: {report['easy_pairs']['count']}\")\nprint(f\"Exact duplicates: {report['dedup']['exact_duplicate_count']}\")\nprint(f\"Near-duplicate pairs: {report['dedup']['near_duplicate_count']}\")\nprint()\nfor rec in report.get(\"recommendations\", []):\n    print(f\"  → {rec}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Filter Dataset Based on Profiler Findings"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from datasets import Dataset\n\n# Collect all flagged indices\nremove = set()\nremove.update(report.get(\"easy_pairs\", {}).get(\"indices\", []))\n\n# Low-contrast pairs (similarity > 0.9)\nes = report.get(\"embedding_similarity\", {})\nif es.get(\"low_contrast_count\", 0) > 0:\n    # Re-run to get per-pair data since the report only has aggregate stats\n    from preflight.loader import load_dataset as pf_load\n    from preflight.embeddings import analyze_embedding_similarity\n    samples, _, _ = pf_load(\"ultrafeedback_train.jsonl\")\n    _, chosen_emb, rejected_emb, sims = analyze_embedding_similarity(samples)\n    import numpy as np\n    low_contrast_idx = np.where(sims > 0.9)[0]\n    remove.update(int(i) for i in low_contrast_idx)\n\nprint(f\"Flagged indices: {len(remove)} / {len(ds_train)}\")\nprint(f\"  Easy pairs: {report.get('easy_pairs', {}).get('count', 0)}\")\nprint(f\"  Low-contrast: {es.get('low_contrast_count', 0)}\")\nprint(f\"  (some may overlap)\")\n\nfiltered_rows = [row for i, row in enumerate(ds_train) if i not in remove]\nds_filtered = Dataset.from_list(filtered_rows)\nprint(f\"\\nFiltered: {len(ds_filtered)} pairs (removed {len(ds_train) - len(ds_filtered)})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. DPO Training Setup"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import DPOConfig, DPOTrainer\nfrom peft import LoraConfig\nimport torch\n\nMODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nlora_config = LoraConfig(\n    r=16, lora_alpha=32, lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n\ndef make_dpo_config(output_dir):\n    return DPOConfig(\n        output_dir=output_dir,\n        num_train_epochs=1,\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=4,\n        learning_rate=5e-5,\n        beta=0.1,\n        logging_steps=10,\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        save_strategy=\"no\",\n        bf16=torch.cuda.is_bf16_supported(),\n        fp16=not torch.cuda.is_bf16_supported(),\n        report_to=\"none\",\n    )",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Train on FULL dataset ---\nmodel_full = AutoModelForCausalLM.from_pretrained(\n    MODEL, torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n    device_map=\"auto\",\n)\n\ntrainer_full = DPOTrainer(\n    model=model_full,\n    args=make_dpo_config(\"./dpo_full\"),\n    train_dataset=ds_train,\n    eval_dataset=ds_eval,\n    processing_class=tokenizer,\n    peft_config=lora_config,\n)\n\ntrainer_full.train()\nfull_logs = trainer_full.state.log_history\nprint(f\"Full dataset training complete — {len(ds_train)} pairs\")\n\ndel model_full, trainer_full\ntorch.cuda.empty_cache()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Train on FILTERED dataset ---\nmodel_filt = AutoModelForCausalLM.from_pretrained(\n    MODEL, torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n    device_map=\"auto\",\n)\n\ntrainer_filt = DPOTrainer(\n    model=model_filt,\n    args=make_dpo_config(\"./dpo_filtered\"),\n    train_dataset=ds_filtered,\n    eval_dataset=ds_eval,\n    processing_class=tokenizer,\n    peft_config=lora_config,\n)\n\ntrainer_filt.train()\nfiltered_logs = trainer_filt.state.log_history\nprint(f\"Filtered dataset training complete — {len(ds_filtered)} pairs\")\n\ndel model_filt, trainer_filt\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import matplotlib.pyplot as plt\n\ndef get_series(logs, key):\n    return [(l[\"step\"], l[key]) for l in logs if key in l]\n\ndef get_eval_series(logs, key):\n    return [(l[\"step\"], l[key]) for l in logs if key in l and \"eval\" in key]\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. Eval reward accuracy\nfor name, logs in [(\"Full (4800)\", full_logs), (\"Filtered\", filtered_logs)]:\n    data = get_series(logs, \"eval_rewards/accuracies\")\n    if data:\n        steps, vals = zip(*data)\n        axes[0].plot(steps, vals, marker=\"o\", label=name)\naxes[0].set_title(\"Eval Reward Accuracy\")\naxes[0].set_xlabel(\"Step\")\naxes[0].set_ylabel(\"Accuracy\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# 2. Eval loss\nfor name, logs in [(\"Full (4800)\", full_logs), (\"Filtered\", filtered_logs)]:\n    data = get_series(logs, \"eval_loss\")\n    if data:\n        steps, vals = zip(*data)\n        axes[1].plot(steps, vals, marker=\"o\", label=name)\naxes[1].set_title(\"Eval Loss\")\naxes[1].set_xlabel(\"Step\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# 3. Train loss\nfor name, logs in [(\"Full (4800)\", full_logs), (\"Filtered\", filtered_logs)]:\n    data = get_series(logs, \"loss\")\n    if data:\n        steps, vals = zip(*data)\n        axes[2].plot(steps, vals, alpha=0.8, label=name)\naxes[2].set_title(\"Train Loss\")\naxes[2].set_xlabel(\"Step\")\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.suptitle(\"DPO Training: Full vs. Preflight-Filtered Dataset\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.savefig(\"comparison.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Final metrics summary\ndef last_eval(logs, key):\n    vals = [l[key] for l in logs if key in l]\n    return vals[-1] if vals else None\n\nfull_acc = last_eval(full_logs, \"eval_rewards/accuracies\")\nfilt_acc = last_eval(filtered_logs, \"eval_rewards/accuracies\")\nfull_loss = last_eval(full_logs, \"eval_loss\")\nfilt_loss = last_eval(filtered_logs, \"eval_loss\")\n\nprint(\"=\" * 50)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\" * 50)\nprint(f\"{'Metric':<30} {'Full':>8} {'Filtered':>8}\")\nprint(\"-\" * 50)\nprint(f\"{'Train samples':<30} {len(ds_train):>8} {len(ds_filtered):>8}\")\nprint(f\"{'Removed pairs':<30} {'—':>8} {len(ds_train) - len(ds_filtered):>8}\")\nif full_acc is not None:\n    print(f\"{'Eval reward accuracy':<30} {full_acc:>8.4f} {filt_acc:>8.4f}\")\n    delta = filt_acc - full_acc\n    print(f\"{'Δ accuracy':<30} {'':>8} {delta:>+8.4f}\")\nif full_loss is not None:\n    print(f\"{'Eval loss':<30} {full_loss:>8.4f} {filt_loss:>8.4f}\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "source": "## Interpretation\n\n**What this shows**: Whether removing preflight-flagged pairs (easy, low-contrast, duplicates) from UltraFeedback improves DPO training on a held-out eval set.\n\n**What to look for**:\n- Higher eval reward accuracy for filtered → profiling helps\n- Lower eval loss for filtered → better generalization\n- If filtered trains on fewer samples but matches or beats full → data quality > data quantity\n\n**Limitations at this scale**:\n- Single dataset (UltraFeedback), single model (1.5B), single seed\n- Small held-out set (200 pairs) — eval variance is high\n- 1 epoch of training — longer training may change the picture\n\n**With more compute, extend to**:\n- Multiple datasets (UltraFeedback, Nectar, HH-RLHF)\n- Multiple model sizes (1.5B, 7B)\n- Multiple seeds + confidence intervals\n- MT-Bench / AlpacaEval generation quality eval",
   "metadata": {}
  }
 ]
}