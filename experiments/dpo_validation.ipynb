{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Data Quality Validation Experiment\n",
        "This notebook validates whether preflight's data quality profiling improves DPO training outcomes.\n",
        "\n",
        "**Setup:** Run on Google Colab with T4/A100 GPU runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install preflight sentence-transformers trl transformers datasets peft accelerate bitsandbytes -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 - Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=\"train_prefs[:5000]\")\n",
        "print(f\"Loaded {len(ds)} preference pairs\")\n",
        "\n",
        "# Save as JSONL for preflight\n",
        "import json\n",
        "with open(\"ultrafeedback_5k.jsonl\", \"w\") as f:\n",
        "    for row in ds:\n",
        "        f.write(json.dumps({\n",
        "            \"prompt\": row[\"prompt\"],\n",
        "            \"chosen\": row[\"chosen\"],\n",
        "            \"rejected\": row[\"rejected\"],\n",
        "        }) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 - Run Preflight Audit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!preflight audit ultrafeedback_5k.jsonl --output audit_report.json\n",
        "\n",
        "import json\n",
        "with open(\"audit_report.json\") as f:\n",
        "    report = json.load(f)\n",
        "\n",
        "print(\"\\n=== Key Findings ===\")\n",
        "for rec in report.get(\"recommendations\", []):\n",
        "    print(f\"  - {rec}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 - Create Filtered Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the report to get indices to remove\n",
        "easy_indices = set(report.get(\"easy_pairs\", {}).get(\"indices\", []))\n",
        "print(f\"Removing {len(easy_indices)} easy pairs\")\n",
        "\n",
        "# Create filtered dataset\n",
        "filtered_rows = [row for i, row in enumerate(ds) if i not in easy_indices]\n",
        "print(f\"Filtered dataset: {len(filtered_rows)} pairs (from {len(ds)})\")\n",
        "\n",
        "from datasets import Dataset\n",
        "ds_filtered = Dataset.from_list(filtered_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 - DPO Training (Full vs Filtered)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from peft import LoraConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# LoRA config for memory efficiency\n",
        "lora_config = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Training config\n",
        "training_args = DPOConfig(\n",
        "    output_dir=\"./dpo_full\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    beta=0.1,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.bfloat16, device_map=\"auto\",\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Full dataset training complete\")\n",
        "# Save final reward accuracy\n",
        "full_metrics = trainer.state.log_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reset model\n",
        "del model, trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=torch.bfloat16, device_map=\"auto\",\n",
        ")\n",
        "\n",
        "training_args.output_dir = \"./dpo_filtered\"\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_filtered,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Filtered dataset training complete\")\n",
        "filtered_metrics = trainer.state.log_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 - Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_metric(logs, key):\n",
        "    return [(l[\"step\"], l[key]) for l in logs if key in l]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Reward accuracy\n",
        "for name, metrics in [(\"Full\", full_metrics), (\"Filtered\", filtered_metrics)]:\n",
        "    data = extract_metric(metrics, \"rewards/accuracies\")\n",
        "    if data:\n",
        "        steps, vals = zip(*data)\n",
        "        axes[0].plot(steps, vals, label=name)\n",
        "axes[0].set_title(\"Reward Accuracy\")\n",
        "axes[0].set_xlabel(\"Step\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Loss\n",
        "for name, metrics in [(\"Full\", full_metrics), (\"Filtered\", filtered_metrics)]:\n",
        "    data = extract_metric(metrics, \"loss\")\n",
        "    if data:\n",
        "        steps, vals = zip(*data)\n",
        "        axes[1].plot(steps, vals, label=name)\n",
        "axes[1].set_title(\"Training Loss\")\n",
        "axes[1].set_xlabel(\"Step\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"comparison.png\", dpi=150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "| Metric | Full Dataset | Filtered Dataset |\n",
        "|--------|-------------|------------------|\n",
        "| Final reward accuracy | _fill in_ | _fill in_ |\n",
        "| Final loss | _fill in_ | _fill in_ |\n",
        "| Training time | _fill in_ | _fill in_ |\n",
        "| Dataset size | 5000 | _fill in_ |\n",
        "\n",
        "### Key Finding\n",
        "_Fill in after running: did filtering improve DPO training?_"
      ]
    }
  ]
}
